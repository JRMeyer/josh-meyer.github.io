---
layout: post
title:  "Kaldi on AWS"
date:   2017-10-13
categories: ASR
comments: True
redirect_from: /kaldi/2017/10/13/Kaldi-AWS.html
---

<br/>
<br/>

> üëã Hi, it's Josh here. I'm writing you this note in 2021: the world of speech technology has changed dramatically since Kaldi. Before devoting weeks of your time to deploying Kaldi, take a look at üê∏ [Coqui Speech-to-Text][coqui-github]. It takes minutes to deploy an off-the-shelf üê∏ STT model, and it's [open source on Github][coqui-github]. I'm on the Coqui founding team so I'm admittedly biased. However, you can tell from this blog that I've spent years working with Kaldi, so I understand the headaches.
>
> With üê∏ STT, we've removed the headaches of Kaldi and streamlined everything for production settings. You can train and deploy state-of-the-art üê∏ Speech-to-Text models in just minutes, not weeks. Check out the [üê∏ Model Zoo][coqui-model-zoo] for open, pre-trained models in different languages. Try it out for yourself, and come join our [friendly chatroom][coqui-gitter] üíö

<br/>
<br/>
<br/>
<br/>

<img src="/misc/kaldi_text_and_logo.png" align="left" style="height: 95px"/>
<img src="/misc/right-arrow.svg" align="middle" style="height: 80px"/>
<img src="/misc/aws-logo.svg" align="right" style="height: 90px"/>

<br/>

## General Notes

1. Nice [intro to Amazon's EC2][ec2] services
2. Use `tmux`: nice for running a model in background after exiting `ssh`
3. Install `emacs` or `vim`: you're going to need some kind of text editor which works well in the terminal.
4. Installing Kaldi on `t2.micro` will run out of RAM and crash, so you're going to have to pay.


<br/>
<br/>

## Set up Instance

### Choose Instance

You can only launch instances which you have permissions for, find the list of available instances here:

<br/>
<img src="/misc/aws-screenshot.png" align="left" alt="logo" style="width: 800px"/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>

### Instance Info

`https://aws.amazon.com/ec2/instance-types/`


### Instance Pricing

`https://aws.amazon.com/ec2/pricing/on-demand/`


### My Set-up

I've been working on the following set-up, so it should work for you as well.

1. operating system: `Ubuntu Server 16.04 LTS`
2. instance type: `g2.2xlarge`

The instance type I'm using is for GPUs, which is probably what you want... especially if you're using `nnet3`. It's more expensive than using a CPU, but if you're running lots of data or experiments, you will want GPUs. Each iteration in DNN training takes a lot less time on a GPU.

If you really want to use a CPU system, I'd recommend `c4.xlarge`. Actually, if you're only running GMMs, then more CPUs is better than a single GPU, since GMMs don't train on GPUs, and it's easier to parallelize GMMs so you get more bang for your buck with a bunch of CPUs.

Don't forget: a standard, compiled Kaldi will take up to `15 gigs` of disk space, so make sure you allocate it on the instance when you're setting it up (on the `Storage` step). This is why you should also really use `./configure --shared` below, it will shave off some gigs.


### Download Key Pair

Generate a key-pair, and make it secure with `chmod` (this is a necessary step).

Don't lose this file, or you won't be able to get back into your instance!

{% highlight bash %}
chmod 400 your-key-pair.pem
{% endhighlight %}


### SSH into instance

Now use your key-pair and `ssh` into your instance.

{% highlight bash %}
ssh -i "your-key-pair.pem" ubuntu@ec2-11-222-33-444.us-west-2.compute.amazonaws.com
{% endhighlight %}


<br/>
<br/>

### Update and Upgrade Ubuntu

This is necessary to install Kaldi successfully.

{% highlight bash %}
sudo apt-get update; sudo apt-get upgrade
{% endhighlight %}

If you've gotten to this point without any hiccups, you should have a working `Ubuntu` installation on your `AWS` instance! Now, let's quickly download and compile `Kaldi` on our new instance:)

<br/>
<br/>

### Install Dependencies

{% highlight bash %}
sudo apt-get install g++ make automake autoconf python zlib1g-dev make automake autoconf libtool subversion libatlas3-base sox bc emacs24 tmux
{% endhighlight %}


<br/>
<br/>

### Install CUDA

I got these steps from this [good post on askubuntu][install-cuda]. If you have issues, that post is more detailed, check it out. Normally, though, these steps should work.

First download CUDA from NVIDIA. You might need a different link here depending on when you read this.

{% highlight bash %}
sudo wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda_9.0.176_384.81_linux-run
{% endhighlight %}

Install CUDA:

{% highlight bash %}
sudo sh cuda_9.0.176_384.81_linux-run --override
{% endhighlight %}

Add CUDA to your `PATH` variable:

{% highlight bash %}
echo "export PATH=$PATH:/usr/local/cuda-9.0/bin" >> ~/.profile
source ~/.profile
{% endhighlight %}

Take a look at your install:

{% highlight bash %}
nvcc --version
{% endhighlight %}

Once you're training, you can look at GPU usage (like `top` for CPUs) like this:

{% highlight bash %}
watch nvidia-smi
{% endhighlight %}

<br/>
<br/>

### Install Kaldi on Instance

The following is a list of commands, assuming you've already had experience installing Kaldi. However, if you're interested, here's a thorough [walk-through on installing Kaldi][install-kaldi].

### Clone Kaldi

{% highlight bash %}
git clone https://github.com/kaldi-asr/kaldi.git
{% endhighlight %}

### Compile 3rd Party Tools
{% highlight bash %}
ubuntu@ip-172-31-35-58:~/kaldi/tools$ make -j `nproc`
ubuntu@ip-172-31-35-58:~/kaldi/tools$ extras/install_irstlm.sh
{% endhighlight %}


### Compile Kaldi Source
{% highlight bash %}
# use --shared to use shared vs static links... saves a TON of
# space on disk (thanks to Fran Tyers for pointing this out to me!)
ubuntu@ip-172-31-35-58:~/kaldi/src$ ./configure --shared
ubuntu@ip-172-31-35-58:~/kaldi/src$ make depend -j `nproc`
ubuntu@ip-172-31-35-58:~/kaldi/src$ make -j `nproc`
{% endhighlight %}

At this point, if you didn't get any `ERROR` messages or major `WARNING`s, you should have a working installation of `Kaldi` in the cloud!

To make sure you've go everything working, go ahead and run the simple `yesno` demo:

### Test it out!
{% highlight bash %}
ubuntu@ip-172-31-35-58:~/kaldi/src$ cd ../egs/yesno/s5
ubuntu@ip-172-31-35-58:~/kaldi/egs/yesno/s5$ ./run.sh
{% endhighlight %}

If it runs as normal, you're good to go:)

Now you can tranfer your own data to your instance and start working. You should probably check out [Amazon's EBS volumes][ebs] for storing your data, but that's another post. Here, I'm just going to show you how to transfrom data from your local computer to your new instance, and then you should be able to train models as if they were on your machine.

<br/>
<br/>

## Transfer Data

### Transfer Data from Laptop to Instace

You can easily transfer data from your local machine to your new `AWS` instance. Open a terminal on your machine and do something like this:
{% highlight bash %}
scp -i ~/Desktop/your-key-pair.pem ~/Desktop/your-local-data.tar.gz ubuntu@ec2-11-222-33-444.us-west-2.compute.amazonaws.com:~/data/
{% endhighlight %}


### Attach an EBS Volume

This is from what I gather the best way to work with data on AWS for these kinds of applications (machine learning training).

If you have your data on an EBS volume, it is logically separate from your scripts and codebase on EC2 instance. Splitting data and code is good practice, especially if you're using git and GitHub.

My code for speech recognition experiements is in one git repo, and I can easily spin up an EC2 instance, clone my repo, and use symbolic links to my data on EBS after I've mounted it. This way, you can plug and play with different data sets, or code bases.

Also, git will get mad at you iff your files are too big or if your repo is too big. You should really only have text files on git.

1. create EBS Volume in same availability zone (not just region!) as your EC2
2. put data on EBS Volume

Then you need to:

1. stop EC2 instance
2. attach EBS to EC2
3. mount EBS from within a ssh session on EC2



<br/>
<br/>



## Train Models

If you want a real walk-through, check out my post on how to [train a DNN in Kaldi][train-kaldi]. The following is more of a cheatsheet than anything.

### The Quick and Dirty

1. `ssh -i ~/Desktop/your-key-pair.pem ubuntu@ec2-11-222-33-444.us-west-2.compute.amazonaws.com`
2. start new terminal session: `tmux new -s my-session`
3. run model: `KALDI$ ./run.sh`
4. exit terminal session without terminating: `Ctrl-b d`
5. get back into terminal session: `tmux a -t my-session`



{% highlight bash %}
{% endhighlight %}

<br>

## Conclusion

I hope this was helpful! 

Let me know if you have comments or suggestions and you can always leave a comment below. 

Happy Kaldi-ing!

[install-kaldi]: http://jrmeyer.github.io/kaldi/2016/01/26/Installing-Kaldi.html
[train-kaldi]: http://jrmeyer.github.io/kaldi/2016/12/15/DNN-AM-Kaldi.html
[ebs]: https://www.youtube.com/watch?v=DKftR47Ljvw
[ec2]: https://www.youtube.com/watch?v=Px7ZPLq4AOU
[install-cuda]: https://askubuntu.com/questions/799184/how-can-i-install-cuda-on-ubuntu-16-04